{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "# PART 2 of 3. In this notebook, using a real world example, I detail how to estimate price elasticity of demand and revenue elasticity.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "In this use case we will examine price elasticity for a regional retailer, with 219 locations.  The type of retailer isn't important.  It could be a convenience store, a restaurant or coffee shop.  What ever the business of the retailer, their data should look basically the same.  \n\nThe success of a retailer will depend on several factors.  One, is management and management decsions.  Pricing, for example, is 100% controllable by management.  Other factors that are less controllable are the enviromental factors surrounding the store.  For example, a retailer will typically do better if the people living around the store make a bunch a money.  \n\nThe data set we use in this exercise has a combination of controllable and environmental factors.  The controlable factor we are interested in is price.  We also have numerous environmental factors.  \n\nOur goal in this exercise is to better understand the relationship between price, quantity and revenue. We achieve this goal by estimating the price elasticity of demand and the price elasticity of revenue.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# Import data and data transformations", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "First, let's bring in the python libraries we will need in this exercise.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "\nimport sys\nimport types\nimport statsmodels.api as sm\nimport pandas as pd\nimport numpy as np\nfrom botocore.client import Config\nimport ibm_boto3\n\ndef __iter__(self): return 0\n\n!pip install plotly --upgrade\nimport plotly\n#PUT YOUR PLOTLY CREDENTIALS HERE\nplotly.tools.set_credentials_file(username='XXXXXXX', api_key='XXXXXXX')\nimport plotly.plotly as py\nimport plotly.graph_objs as go"
        }, 
        {
            "source": "The next step is to upload the pricing data into object storage.  To do this, click on the data icon (it is the one with two 1's and two 0's) in the northeast corner of the Watson Studio interface. Next, on the \"Files\" tab click on \"Browse\" where it says, \"Drop your file here or browse\n your files to add a new file\". From there, navigate to our data set \"RETAIL_DATA.csv\" and upload it to the cloud.  \n \n Once the file is uploaded it should appear in the data tab.  From the drop down menu, select insert credentials. It should look something like this.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# The code was removed by Watson Studio for sharing."
        }, 
        {
            "source": "The next snippet of code will convert the file you just uploaded to object storage into a pandas dataframe.  Replace the credentials below with the credentials you inserted in the cell above. ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# The code was removed by Watson Studio for sharing."
        }, 
        {
            "source": "The first few rows of the data set should appear above.  \n\nHere is a definition of each field.\n\nSTORE_ID -- is a unique id specific to each retail outlet\n\nPERCENTAGE_OF_RENTERS is the percentage of households surrounding the store that rent their housing.\n\nPERCENTAGE_OF_CHILDREN is the percentage of households surrounding the store that have children.\n\nAVERAGE_INCOME is the average annual income of the households surrounding the store.\n\nAVERAGE_AGE_IN_YEARS is the average age of the head of household in the vacinity of the retail outlet.\n\nAVERAGE_LENGTH_OF_RESIDENCE is an average of the time individuals surrounding the retail outlet have lived at their current address.\n\nPERCENT_SPEAKING_SPANISH is the percentage of households surrounding the store that speak spanish\n\nPRICE is the average price accross multiple items sold at the retail outlet.\n\nQUANTITY is the number of items sold by the retail outlet in the last year.\n\nREVENUE is the total revenue for the store in the last year.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Our goal in this exercise is to calculate the price elasticity of demand and the price elasticity of revenue.  (Please look at the first notebook in this series to understand what these terms are.)  This actually isn't that hard.  To estimate the estimate an elasticity, you can use a standard ordinary least squares regression and natural log (base e)  transformed variables.  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "The first step is to take the natural log each variable.  The cell below does this. Note the new variables appended to the dataframe.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "df_retail['LN_PRICE'] = np.log((df_retail.PRICE))\ndf_retail['LN_REVENUE'] = np.log((df_retail.REVENUE))\ndf_retail['LN_QUANTITY'] = np.log((df_retail.QUANTITY))\ndf_retail['LN_INCOME'] = np.log((df_retail.AVERAGE_INCOME))\ndf_retail['LN_AVERAGE_AGE_IN_YEARS'] = np.log((df_retail.AVERAGE_AGE_IN_YEARS))\ndf_retail['LN_AVERAGE_LENGTH_OF_RESIDENCE'] = np.log((df_retail.AVERAGE_LENGTH_OF_RESIDENCE))\ndf_retail['LN_PERCENT_SPEAKING_SPANISH'] = np.log((df_retail.PERCENT_SPEAKING_SPANISH))\ndf_retail['LN_PERCENT_HAVING_CHILDREN'] = np.log((df_retail.PERCENT_HAVING_CHILDREN))\ndf_retail['LN_PERCENTAGE_OF RENTERS'] = np.log((df_retail.PERCENTAGE_OF_RENTERS))\n\ndf_retail.head()\n"
        }, 
        {
            "source": "# Build the Model", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Now, let's build an ordinary least squares regression using the log transformed variables. \n\nDefine your independent and dependent variables with the following code cell.  Note that ONLY variables that are statistically significant are included.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "independentx = df_retail[['LN_PRICE','LN_PERCENTAGE_OF RENTERS','LN_PERCENT_HAVING_CHILDREN','LN_INCOME',\n                          'LN_PERCENT_SPEAKING_SPANISH']]\nindependent = sm.add_constant(independentx, prepend=False)\ndependent=df_retail['LN_QUANTITY']"
        }, 
        {
            "source": "The next few cells run the OLS regression", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "mod = sm.OLS(dependent, independent)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "res = mod.fit()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "print(res.summary())"
        }, 
        {
            "source": "# Interpreting the OLS Regression Coefficients", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "\n\nSo, the price elasticity of demand is -.64.  This comes the ANOVA table above and is the estimated coefficient of LN_PRICE regressed on LN_QUANTITY. This means that a 1% increase in price will lower quantity sold by .64%.  The other coefficients can be interpreted in a similar manner.  A 1% increase in the  average income of people around a store will increase the quantity sold by .55%.\n\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# Creating a Demand Curve", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Now, let's examine what the demand curve looks like.  To do this, we will need to build a demand schedule using the predicted quantity at various prices based on our OLS regression model.\n\nFirst, let's summarize the variables other than price that are significant in the model.  We will take the average and then evaluate the relationship between price and quanity when the other variables are at their mean.\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "df_retail['chachacha']=1\nwookie = df_retail.groupby(['chachacha'])['PERCENTAGE_OF_RENTERS', 'PERCENT_HAVING_CHILDREN','AVERAGE_INCOME','PERCENT_SPEAKING_SPANISH'].mean()\n\nwookie.reset_index(level=0, inplace=True)\nwookie.head()\n\n\n\n"
        }, 
        {
            "source": "Next, we will create an array of prices and then convert the list to a pandas dataframe.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#create an array of prices\nprice = [1.50,1.75,2.0,2.25,2.50,2.75,3.0,3.25,3.50,3.75,4.0,4.25,4.50,4.75,5.0,5.25,5.50,5.75,6.0,6.25,6.50,6.75,\n         7.0,7.25,7.5,7.75,8.0,8.25,8.50,8.75,9.0,9.25,9.50,9.75,10.0,10.25,10.50,10.75,11.0,11.25,11.50,11.75,12.0]\ndf_price=pd.DataFrame(price)\ndf_price.columns = ['PRICE']\ndf_price['chachacha']=1\n"
        }, 
        {
            "source": "Then, merge the average value for the non-price variables to the prices we constructed above and calculate the log of our variables.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#join the array of prices to the average values of the other independent variables.\ndf_price =df_price.merge(wookie, on=['chachacha'], how='inner')\n\n#Create Log Transformed Variables\ndf_price['LN_PRICE'] = np.log((df_price.PRICE))\ndf_price['LN_INCOME'] = np.log((df_price.AVERAGE_INCOME))\ndf_price['LN_PERCENT_SPEAKING_SPANISH'] = np.log((df_price.PERCENT_SPEAKING_SPANISH))\ndf_price['LN_PERCENT_HAVING_CHILDREN'] = np.log((df_price.PERCENT_HAVING_CHILDREN))\ndf_price['LN_PERCENTAGE_OF RENTERS'] = np.log((df_price.PERCENTAGE_OF_RENTERS))\ndf_price['const']=1\n\ndf_price.head()\n\n\n"
        }, 
        {
            "source": "Now, we can use the input variables we manufactured in the previous few lines of code and our model to predict quantity at each price point.  The end result will be a demand schedule.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#create our scoring data set.\nscoring= df_price[['LN_PRICE','LN_PERCENTAGE_OF RENTERS','LN_PERCENT_HAVING_CHILDREN','LN_INCOME',\n                          'LN_PERCENT_SPEAKING_SPANISH','const']]\n#score the scoring data set\nln_q_hat=pd.DataFrame(res.predict(scoring))\n#name the columns correctly\nln_q_hat.columns = ['LN_Q_HAT']\n#combine ln of price and ln of predicted q into a new data frame\ndf_ce_demand = pd.concat([scoring['LN_PRICE'], ln_q_hat], axis=1)\n\n\n#exponentiate the ln variables to get predicted quantity and price\ndf_ce_demand['Q_HAT']=np.exp(df_ce_demand['LN_Q_HAT'])\ndf_ce_demand['PRICE']=np.exp(df_ce_demand['LN_PRICE'])\n\n#eliminate the ln variables and make the demand schedule.\ndf_ce_demand = df_ce_demand[['Q_HAT','PRICE']]\n\ndf_ce_demand.head()"
        }, 
        {
            "source": "Bingo!  Now we have a demand schedule we can plot as a demand curve.\n\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import plotly.plotly as py\nimport plotly.graph_objs as go\n\ntrace = go.Scatter(\n    x = df_ce_demand['PRICE'],\n    y = df_ce_demand['Q_HAT'],\n    mode = 'lines'\n)\n\n\n\nlayout = go.Layout(\n    title='Demand Curve',\n    xaxis=dict(\n        title='Price',\n        titlefont=dict(\n            family='Courier New, monospace',\n            size=18,\n            color='#7f7f7f'\n        )\n    ),\n    yaxis=dict(\n        title='Quantity',\n        titlefont=dict(\n            family='Courier New, monospace',\n            size=18,\n            color='#7f7f7f'\n        )\n    )\n)\n    \ndata=[trace]  \nfig = go.Figure(data=data, layout=layout)\n\n#plot_url = py.plot(fig, filename='styling-names')\npy.iplot(fig, filename='shapes-lines')"
        }, 
        {
            "source": "Wait a minute!  That's not linear!  Nope, it isn't.  This a variation of the demand curves we messed around with in part 1 of our exercise.  This is what we call a constant elasticity demand curve.  The elasticity is the same, at all points.  Remember in our early discussion that elasticity was different at each price point of the demand curve.  With this demand curve, elasticity is the same at all the price points.  It still shows the relationship between price and quantity.  For example, at a price of 6 dollars, our firm can sell about 75,000 units at each store.\n\nIf you wanted a linear demand curve, you can definitely get there.  You would regress price on quantity (instead of ln of price on ln of quantity).  Of course, if you built your model this way, the coefficient wouldn't be an elasticity.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# Estimate Price Elasticity of Revenue", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Estimating Price Elasticity of Revenue follows a similar process.  The difference is we will use the natural log of Revenue as a dependent variable instead of the natural log of Quantity.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "independentx = df_retail[['LN_PRICE','LN_PERCENTAGE_OF RENTERS','LN_PERCENT_HAVING_CHILDREN','LN_INCOME',\n                          'LN_PERCENT_SPEAKING_SPANISH']]\nindependent = sm.add_constant(independentx, prepend=False)\ndependent=df_retail['LN_REVENUE']"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "mod = sm.OLS(dependent, independent)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "res = mod.fit()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "print(res.summary())"
        }, 
        {
            "source": "The regression results suggest that price elasticity of revenue elasticity is .35.  This means that a 1% increase in price will lead to a .35% increase in revenue.  In other words, this firm would make more revenue if it increased prices.  \n\nThere are few important caveats that I should probably mention.  \n\nOne, this is a point estimate.  That is a fancy way of saying that you shouldn't get too crazy.  If you increase prices by 1% you probably will increase revenue by.35%.  However, if you increase prices by 100%, you probably would not realize a 35% increase in revenue.  Baked into the model is an established historical relationship between your customers and your prices.  If you do something that is way outside of the historical norm, don't expect the model to be predictive.   \n\nTwo, it is important to understand this elasticity is an average accross all stores.  The price elasticity of revenue is .35, on average.  There are 291 stores in the data set.  Some probably have an elasticity greater than .35.  Others probably have an elasticity that is less than .35.  In other words, if you increase prices by 3% across the board, on average,  you will realize a 1.05% increase in revenue.  This is an average.  Some stores will realize more than 1.05%.  Others will realize less than 1.05%. A 3% increase in prices may even cause some stores to lose revenue.  \n\n\nWhat if you could taylor the price increase for each store? That is, increase prices by an average of 3%, but give some stores a higher bump in prices than others.  You could even decrease prices in some stores if it makes sense.  Tayloring the price increase to each store based on their specific market, will lead to an even greater increase in revenue.  For example, you can raise prices by 3% on average and get a increase in revenue greater than 1.05%.  \n\nThere are many ways to accomplish this goal.  In the third part of this exercise, we will examine a relatively simple and straight forward way to make a market based pricing decision for each of our 291 stores.\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": ""
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": ""
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5", 
            "name": "python3", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.5", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}